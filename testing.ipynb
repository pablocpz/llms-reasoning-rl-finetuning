{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf5e0ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# class Model(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         # each token directly reads off the logits from the head of the transformer\n",
    "#         self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "#         self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        \n",
    "#         #the transformer blocks\n",
    "#         self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "        \n",
    "#         #final layer normalization and output head\n",
    "#         self.ln_f = nn.LayerNorm(n_embd)\n",
    "#         self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "    \n",
    "        \n",
    "#     def forward(self, x):\n",
    "        \n",
    "#         B,T = x.shape\n",
    "        \n",
    "#         #token and position embeddings\n",
    "#         tok_emb = self.token_embedding_table(x)\n",
    "#         pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        \n",
    "#         x = tok_emb + pos_emb\n",
    "#         #adding the token and position embeddings together\n",
    "#         #this is the input to the transformer blocks\n",
    "        \n",
    "#         x = self.blocks(x)\n",
    "        \n",
    "#         x = self.ln_f(x)\n",
    "#         #final layer normalization\n",
    "        \n",
    "#         logits = self.lm_head(x)\n",
    "#         #final output head\n",
    "        \n",
    "#         return logits\n",
    "    \n",
    "    \n",
    "#         #cross entropy loss between the logits and the target labels\n",
    "#         #the logits are the predicted probabilities for each token in the vocabulary\n",
    "        \n",
    "#         #the target labels are the original input sequence shifted by one token\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e97df3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_training_script import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a9536c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_transformer = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a84a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39494fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tensor = torch.randint(0, 10, (8, 2), dtype=torch.long)\n",
    "\n",
    "\n",
    "#8 batches of 2 tokens each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d82d76e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_out = my_transformer(sample_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68d5e576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f842a467",
   "metadata": {},
   "source": [
    "We're feeding into the transformer 8 batches of 2 tokens each (without getting embedded yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90c7618f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 65])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cf8f14",
   "metadata": {},
   "source": [
    "The model gives back for each batch, 65 projected dimensions for each of the two input tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f27511b",
   "metadata": {},
   "source": [
    "If we look at the first bach, and first token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bc490f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3468,  0.4618, -0.2735,  1.1536,  0.1317,  0.0072, -0.1471, -1.0126,\n",
       "         0.4182, -1.8540,  0.1052, -0.3743, -0.7551,  0.0751,  0.3399, -0.0829,\n",
       "         0.2060,  0.1405,  0.0172,  0.6288,  0.5224,  0.1173, -0.0275,  0.1603,\n",
       "         0.9017,  0.9181, -0.5283,  0.2505,  0.0328,  1.2271, -0.0290,  0.8465,\n",
       "        -0.1579, -0.5061,  0.3963, -0.7360,  1.0731, -0.4794,  0.6444,  0.2507,\n",
       "        -0.3141,  0.0967,  0.9214, -1.7433, -0.1829,  0.6784, -1.0910,  1.3969,\n",
       "         0.7533, -0.4334,  0.0108, -0.5884,  0.8921,  0.0922,  0.9656,  0.0855,\n",
       "         0.4563,  0.6521, -0.1786,  0.5899,  0.0889, -0.4589,  0.1294, -0.1297,\n",
       "        -0.0860], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_out[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb9e6aa",
   "metadata": {},
   "source": [
    "This is the raw output of the decoder-only LLM without converting back the transformer blocks outputs to real words "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc41c277",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
